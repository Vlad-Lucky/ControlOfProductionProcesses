{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mosraooMKk-o"
      },
      "outputs": [],
      "source": [
        "from imutils.video import VideoStream\n",
        "from imutils.video import FPS\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import logging\n",
        "import pickle\n",
        "import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import subprocess as sp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import copy\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "QLy9XtEbXl9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85492b86-3365-437b-8d83-d3a8cbb6d919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_DIR = '/content/drive/MyDrive/ai/video_surveillance/'\n",
        "FRAME_DATA_FILE = os.path.join(WORKING_DIR, 'datasets/frame_data.pkl')\n",
        "PEOPLE_DATA_FILE = os.path.join(WORKING_DIR, 'datasets/people_data.pkl')\n",
        "FFMPEG_BIN = \"ffmpeg\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ZSjjkBQ2Xkrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_videos_fast = {'train/train1_resized_fast.mp4': [79168, [1920, 1080]], \n",
        "                        'train/train2_fast.mp4': [120211, [1920, 1080]], \n",
        "                        'test/test_fast.mp4': [74760, [1920, 1080]]\n",
        "                        }"
      ],
      "metadata": {
        "id": "aHO1Q2EZQn09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Предсказание для картинок"
      ],
      "metadata": {
        "id": "n0a7cdb3uRmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5l6')\n",
        "model.classes = [0]\n",
        "model.conf = 0.3"
      ],
      "metadata": {
        "id": "UJCLFSAOuVVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "people_data = {}\n",
        "\n",
        "for dataset, (frames_count, size_img) in datasets_videos_fast.items():\n",
        "    if dataset not in people_data:\n",
        "        people_data[dataset] = {}\n",
        "    command = [FFMPEG_BIN,\n",
        "            '-i', os.path.join(WORKING_DIR, f'datasets/{dataset}'),\n",
        "            '-f', 'image2pipe',\n",
        "            '-pix_fmt', 'rgb24',\n",
        "            '-vcodec', 'rawvideo', '-']\n",
        "    pipe = sp.Popen(command, stdout=sp.PIPE)\n",
        "    for frame_i in tqdm(range(frames_count)):\n",
        "        raw_frame = pipe.stdout.read(size_img[0] * size_img[1] * 3)\n",
        "        frame = np.frombuffer(raw_frame, dtype='uint8')\n",
        "        if not any(frame):\n",
        "            break\n",
        "        frame = frame.reshape((size_img[1], size_img[0], 3))\n",
        "\n",
        "        preds = model(frame).xyxy[0]\n",
        "\n",
        "        if preds.any():\n",
        "            people_data[dataset][frame_i] = preds.tolist()\n",
        "\n",
        "        if frame_i % 1000 == 0:\n",
        "            pickle.dump(people_data, open(PEOPLE_DATA_FILE, 'wb'))\n",
        "    pickle.dump(people_data, open(PEOPLE_DATA_FILE, 'wb'))"
      ],
      "metadata": {
        "id": "msV3azoLwwwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сохранение изображений для обучения yolo модели"
      ],
      "metadata": {
        "id": "kjCBYONLZRq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset, annotations in people_data.items():\n",
        "    if 'test' in dataset:\n",
        "        continue\n",
        "    command = [ FFMPEG_BIN,\n",
        "            '-i', os.path.join(WORKING_DIR, f'datasets/{dataset}'),\n",
        "            '-f', 'image2pipe',\n",
        "            '-pix_fmt', 'rgb24',\n",
        "            '-vcodec', 'rawvideo', '-']\n",
        "    pipe = sp.Popen(command, stdout=sp.PIPE)\n",
        "    frames_count, size_img = datasets_videos_fast[dataset]\n",
        "    for frame_i in tqdm(range(frames_count)):\n",
        "        raw_frame = pipe.stdout.read(size_img[0] * size_img[1] * 3)\n",
        "        if frame_i not in annotations:\n",
        "            continue\n",
        "        frame = np.frombuffer(raw_frame, dtype='uint8')\n",
        "        if not any(frame):\n",
        "            break\n",
        "        frame = frame.reshape((size_img[1], size_img[0], 3))\n",
        "        len_annotations = len(annotations[frame_i])\n",
        "        if len_annotations != 0:\n",
        "            img_name = f'{dataset.split(\"/\")[-1].split(\"_\")[0]}_{frame_i}_{len_annotations}.jpg'\n",
        "            cv2.imwrite(os.path.join(WORKING_DIR, f'datasets/human_detect/images/{img_name}'), frame)"
      ],
      "metadata": {
        "id": "_kENAKuUZV6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = [os.path.join(os.path.join(WORKING_DIR, 'datasets/human_detect/images'), elem)\n",
        "          for elem in os.listdir(os.path.join(WORKING_DIR, 'datasets/human_detect/images')) if '.jpg' in elem]\n",
        "images.sort()\n",
        "\n",
        "train_images = images[20000:]\n",
        "val_images = images[:20000]"
      ],
      "metadata": {
        "id": "vg_VpTqVZJOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def move_files_to_folder(list_of_files, destination_folder):\n",
        "    for f in tqdm(list_of_files):\n",
        "        try:\n",
        "            shutil.move(f, destination_folder)\n",
        "        except:\n",
        "            assert False\n",
        "\n",
        "move_files_to_folder(train_images, os.path.join(WORKING_DIR, 'datasets/human_detect/images/train'))\n",
        "move_files_to_folder(val_images, os.path.join(WORKING_DIR, 'datasets/human_detect/images/val'))"
      ],
      "metadata": {
        "id": "RkJ4KEFvZZM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = os.listdir(os.path.join(WORKING_DIR, 'datasets/human_detect/images/train'))\n",
        "val_images = os.listdir(os.path.join(WORKING_DIR, 'datasets/human_detect/images/val'))\n",
        "for_images = train_images + val_images\n",
        "for dataset, all_annotations in people_data.items():\n",
        "    frames_count, size_img = datasets_videos_fast[dataset]\n",
        "    for frame_i in tqdm(range(frames_count)):\n",
        "        if frame_i not in all_annotations:\n",
        "            continue\n",
        "        annotations = all_annotations[frame_i]\n",
        "        len_annotations = len(annotations)\n",
        "        img_name = f'{dataset.split(\"/\")[-1].split(\"_\")[0]}_{frame_i}_{len_annotations}.jpg'\n",
        "        if img_name in for_images:\n",
        "            annotation_name = f'{dataset.split(\"/\")[-1].split(\"_\")[0]}_{frame_i}_{len_annotations}.txt'\n",
        "            annotations = [[0, \n",
        "                            annotation[0] / size_img[0], \n",
        "                            annotation[1] / size_img[1], \n",
        "                            annotation[2] / size_img[0] - annotation[0] / size_img[0], \n",
        "                            annotation[3] / size_img[1] - annotation[1] / size_img[1]] for annotation in annotations]\n",
        "            annotations = [[annotation[0], \n",
        "                            annotation[1] + annotation[3] / 2, \n",
        "                            annotation[2] + annotation[4] / 2, \n",
        "                            annotation[3], annotation[4]] for annotation in annotations]\n",
        "            text = '\\n'.join(map(lambda x: ' '.join(map(str, x)), annotations))\n",
        "            annotation_name = f'{dataset.split(\"/\")[-1].split(\"_\")[0]}_{frame_i}_{len_annotations}.txt'\n",
        "            if img_name in train_images:\n",
        "                with open(os.path.join(WORKING_DIR, f'datasets/human_detect/labels/train/{annotation_name}'), 'w') as f:\n",
        "                    f.write(text)\n",
        "            else:\n",
        "                with open(os.path.join(WORKING_DIR, f'datasets/human_detect/labels/val/{annotation_name}'), 'w') as f:\n",
        "                    f.write(text)"
      ],
      "metadata": {
        "id": "WqZycGgRaEiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# yolov5 из github"
      ],
      "metadata": {
        "id": "2ay3WZ_saKj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaHKqHkjbkrl",
        "outputId": "df1edf98-7531-4e1c-d5b6-97f130551d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 12171, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 12171 (delta 16), reused 6 (delta 2), pack-reused 12140\u001b[K\n",
            "Receiving objects: 100% (12171/12171), 12.60 MiB | 26.66 MiB/s, done.\n",
            "Resolving deltas: 100% (8373/8373), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"train: ../datasets/human_detect/images/train/\n",
        "val:  ../datasets/human_detect/images/val/\n",
        "nc: 1\n",
        "names: [\"human\"]\"\"\"\n",
        "with open('yolov5/data/human_detect.yaml', 'w') as f:\n",
        "    f.write(data)"
      ],
      "metadata": {
        "id": "E-NX_J7BbxsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# обучать yolov5l6 оказалось слишком долго\n",
        "!python yolov5/train.py --img 640 --cfg yolov5l.yaml --batch 12 --epochs 1 --data human_detect.yaml --weights yolov5l.pt --name yolo_human_detect"
      ],
      "metadata": {
        "id": "sbIqMXYeaM7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}